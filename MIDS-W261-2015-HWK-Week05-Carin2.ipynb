{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment: Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juanjo Carin\n",
    "- [juanjose.carin@ischool.berkeley.edu](mailto:juanjose.carin@ischol.berkeley.com)\n",
    "- W261-2\n",
    "- Week 05\n",
    "- Submission date: 10/06/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Errata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will upload any **minor corrections** I may make to the assignment after I submit it:\n",
    "\n",
    "[https://www.dropbox.com/s/59p1i0sb90qw8cp/HW5-Errata.txt?dl=0](https://www.dropbox.com/s/59p1i0sb90qw8cp/HW5-Errata.txt?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **What is a data warehouse?**\n",
    "\n",
    "2. **What is a Star schema?**\n",
    "\n",
    "3. **When is it used?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A **data warehouse** is a central *repository of* integrated *data* from multiple sources; these (current and historical) data can then be used for reporting and data analysis.\n",
    "\n",
    "2. The **star schema** is a data schema that consists of a fact table (or simply called a *fact*: a transaction, an event, a log entry...) referencing multiple *dimensions* (or dimension tables; e.g., business objects/attributes). Hence we can slice & dice over those different dimensions, thus handling simple queries. So the fact table in the center could contain information about a sale (price, quantity, time...), and reference dimensions such as product (name, brand, category...), store (address, ID...), etc.\n",
    "\n",
    "3. It is **used when**:\n",
    "\n",
    "    * data are not necessarily normalized,\n",
    "    * we just need simple queries and fast aggregation , and/or\n",
    "    * we want to feed OLAP cubes efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. **In the database world What is 3NF? **\n",
    "2. **Does machine learning use data in 3NF? If so why?**\n",
    "3. **In what form does ML consume data?**\n",
    "4. **Why would one use log files that are denormalized?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **3NF** (Third Normal Form) is a normalization (i.e., an organization of data into columns--attributes--and tables--relations-- to minimize redundancies, by decomposing a flat table into smaller relational tables)  in which:\n",
    "    * the relation table is in 2NF:\n",
    "        * every non-prime attribute of the table (i.e., that does not belong to any candidate key of the table) is dependent on the whole of every candidate key)\n",
    "    * every non-prime attribute  of the relation table is non-transitively dependent on every superkey of R.\n",
    "\n",
    "  Requiring existence of \"the key\" ensures that the table is in 1NF; requiring that non-key attributes be dependent on \"the whole key\" ensures 2NF; further requiring that non-key attributes be dependent on \"nothing but the key\" ensures 3NF.\n",
    "\n",
    "2. To solve **Machine Learning** problems we usually do not use data in **3NF** because the information in each table alone does not give the \"full picture:\" we need to **denormalize** the **data** first, joining or aggregating tables, to be able to answer typical questions from a Machine Learning perspective (that involve all the dimensions at hand)\n",
    "\n",
    "3. As mentioned in the previous point, ML algorithms use **denormalized data**. This is because most of those algorithms apply a function repeatedly to the same dataset to optimize a parameter (e.g., through gradient descen), so we need the total amount of information.\n",
    "\n",
    "4. For the reason exposed above: denormalized log files include, for a particular observation (or log file), all the information (variables) we are going to use to apply a ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2 (i.e., transformed log file). In this output please include the webpage URL, webpageID and Visitor ID.)**\n",
    "\n",
    "**Justify which table you chose as the Left table in this hashside join.**\n",
    "\n",
    "**Please report the number of rows resulting from:**\n",
    "\n",
    "1. **Inner joining Table Left with Table Right**\n",
    "\n",
    "2. **Right joining Table Left with Table Right**\n",
    "\n",
    "3. **Left joining Table Left with Table Right**\n",
    "\n",
    "(I've reversed the order mentioned in the Instructions, so each new join adds a bit of complexity over the previous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Left and Right Tables\n",
    "Since I included the URLs in the transformed log file, I will generate both tables from scratch.\n",
    "\n",
    "Recall that the lines in the original file have these form:\n",
    "\n",
    "    ...\n",
    "    A,1100,1,\"MS in Education\",\"/education\"\n",
    "    A,1210,1,\"SNA Support\",\"/snasupport\"\n",
    "    C,\"10001\",10001\n",
    "    V,1000,1\n",
    "    V,1001,1\n",
    "    V,1002,1\n",
    "    C,\"10002\",10002\n",
    "    V,1001,1\n",
    "    V,1003,1\n",
    "    ...\n",
    "\n",
    "I.e., all the webpage IDs (the primary key) are listed with their URLs, and then each visitor ID, followed by the webpages he or she visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Instances  32711\n",
      "Attributes  294\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/' +\\\n",
    "    'anonymous-msweb.data'\n",
    "import os\n",
    "os.chdir('/home/hduser/Dropbox/W261/HW5')\n",
    "# Two counters to keep track of number of distinct webpages and visitors\n",
    "A = 0\n",
    "C = 0\n",
    "\n",
    "with open('TableLeft.txt', 'w') as TL, open('TableRight.txt', 'w') as TR:\n",
    "    for line in urllib2.urlopen(url):\n",
    "        record = line.strip().split(',')\n",
    "        record = [x.strip('\"') for x in record]\n",
    "        # If the record corresponds to an attribute, linke webpage ID with URL\n",
    "        if record[0] == 'A':\n",
    "            A += 1\n",
    "            key = record[1] # webpage ID\n",
    "            value = record[4] # webpage URL\n",
    "            TL.write(key + ',' + value + '\\n')\n",
    "        # If the record corresponds to a case (visitor), save that info...\n",
    "        elif record[0] == 'C':\n",
    "            C += 1\n",
    "            value = record[1]\n",
    "        # ... and pass it to the Vroot (i.e., link visitor ID and webpage ID)\n",
    "        elif record[0] == 'V':\n",
    "            key = record[1]\n",
    "            TR.write(key + ',' + value + '\\n')\n",
    "            \n",
    "print 'Training Instances  {}'.format(C)\n",
    "print 'Attributes  {}'.format(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [https://kdd.ics.uci.edu/databases/msweb/msweb.data.html](https://kdd.ics.uci.edu/databases/msweb/msweb.data.html) there were:\n",
    "\n",
    "`Training Instances  32711\n",
    "Attributes  294`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory analysis of the 2 tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of webpages:         294\n",
      "Number of visitors:         32711\n",
      "Number of webpages visited: 285\n",
      "Number of visits:           98654\n"
     ]
    }
   ],
   "source": [
    "# Number of lines in TableLeft.txt\n",
    "!echo \"Number of webpages:         \"$(cat TableLeft.txt | wc -l)\n",
    "# Number of unique visitor IDs in TableRight.txt\n",
    "!echo \"Number of visitors:         \"$(cat TableRight.txt | cut -d',' -f2 | \\\n",
    "                                      uniq | wc -l)\n",
    "# Number of unique webpage IDs in TableRight.txt\n",
    "    # (sort before finding unique values: they have to be adjacent)\n",
    "!echo \"Number of webpages visited: \"$(cat TableRight.txt | cut -d',' -f1 | \\\n",
    "                                      sort | uniq | wc -l)\n",
    "# Number of lines in TableRight.txt\n",
    "!echo \"Number of visits:           \"$(cat TableRight.txt | wc -l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already saw in HW4, 9 webpages were not visited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.2.1: Inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MRJob task for Inner Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HashSideInnerJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HashSideInnerJoin.py\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "    \n",
    "class HashSideInnerJoin(MRJob):\n",
    "\n",
    "    def mapper_init(self):\n",
    "        # Load left-side table in memory as dictionary\n",
    "        self.TL = {}\n",
    "        # The absolute path will be passed as argument when calling MRJob\n",
    "        for key, value in csv.reader(open(\"TableLeft.txt\", \"r\")):\n",
    "            # key = webpage ID, value = webpage URL\n",
    "            self.TL[key] = value   \n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        # Iterate over the right-side table, a record at a time\n",
    "        TRrecord = line.split(\",\")\n",
    "        key = TRrecord[0]\n",
    "        value_visitor = TRrecord[1]\n",
    "        # Look for each record, in the left-side table (in-memory)\n",
    "        if key in self.TL.keys():\n",
    "            yield key, (self.TL[key], value_visitor)\n",
    "    \n",
    "    # The reducer is optional. If not specified, I found out records are not \n",
    "        # sorted by webpage ID\n",
    "    def reducer(self, key, value):\n",
    "        for val_url, val_visitor in value:\n",
    "            yield key, (val_url, val_visitor)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    HashSideInnerJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Python script to execute any of the 3 types of Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW52.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "import sys\n",
    "JoinType = sys.argv[1]\n",
    "\n",
    "# Import the class\n",
    "\n",
    "if JoinType == 'Inner':\n",
    "    from HashSideInnerJoin import HashSideInnerJoin\n",
    "    JoinClass = 'HashSideInnerJoin'\n",
    "    output = 'InnerJoinTable.txt'\n",
    "elif JoinType == 'Right':\n",
    "    from HashSideRightJoin import HashSideRightJoin\n",
    "    JoinClass = 'HashSideRightJoin'\n",
    "    output = 'RightJoinTable.txt'\n",
    "elif JoinType == 'Left':\n",
    "    from HashSideLeftJoin import HashSideLeftJoin\n",
    "    JoinClass = 'HashSideLeftJoin'\n",
    "    output = 'LeftJoinTable.txt'\n",
    "else:\n",
    "    raise ValueError('USE Inner, Right, OR Left AS ARGUMENTS')\n",
    "    \n",
    "# Use the 2 tables, left-side as seconrd argument (to be load by mapper_init)\n",
    "mr_job = eval(JoinClass)(args=['TableRight.txt', '--file=TableLeft.txt'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    # Create the join table\n",
    "    with open(output,'w') as result:\n",
    "        for line in runner.stream_output():\n",
    "            webpageID = str(mr_job.parse_output_line(line)[0])\n",
    "            # Extract webpage URL and visitor ID from value\n",
    "            webpageURL = mr_job.parse_output_line(line)[1][0]\n",
    "            visitorID = str(mr_job.parse_output_line(line)[1][1])\n",
    "            result.writelines(webpageID + ',' + webpageURL + ',' + visitorID \n",
    "                              +'\\n')\n",
    "    result.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Call Python Script with Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x HW52.py\n",
    "!./HW52.py Inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create bash script for EDA of the joint table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDA_HW52.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDA_HW52.sh\n",
    "joinTable=$1\n",
    "\n",
    "echo \"Number of webpage IDs:                            \"\\\n",
    "    $(cut -d, -f1 $joinTable | grep -v None | sort | uniq | wc -l)\n",
    "echo \"Number of webpage URLs:                           \"\\\n",
    "    $(cut -d, -f1,2 $joinTable | grep -v None | sort  | uniq | wc -l)\n",
    "echo \"Number of webpages with no associated webpage URL:\"\\\n",
    "    $(cut -d, -f1,2 $joinTable | grep None | sort | uniq | wc -l)\n",
    "echo \"Number of webpages visited:                       \"\\\n",
    "    $(cut -d, -f1,3 $joinTable | grep -v None | cut -d, -f1 | sort | uniq | \\\n",
    "      wc -l)\n",
    "echo \"Number of records:                                \"\\\n",
    "    $(wc -l < $joinTable)\n",
    "echo \"Number of visits:                                 \"\\\n",
    "    $(cut -d, -f3 $joinTable | grep -v None | wc -l)\n",
    "echo \"Number of webpages with no associated visitor ID: \"\\\n",
    "    $(cut -d, -f1,3 $joinTable | grep None | sort | uniq | wc -l)\n",
    "echo \"Number of visitors (IDs):                         \"\\\n",
    "    $(cut -d, -f3 $joinTable | grep -v None | sort | uniq | wc -l)\n",
    "if [ $(grep None $joinTable | wc -l) != 0 ]; then \\\n",
    "    echo -e \"Webpages with no visits or URL:\\n$(grep None $joinTable | \\\n",
    "    sort | sed 's/^/\\t/')\"; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###EDA and output of the inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory analysis of the joint table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of webpage IDs:                             285\n",
      "Number of webpage URLs:                            285\n",
      "Number of webpages with no associated webpage URL: 0\n",
      "Number of webpages visited:                        285\n",
      "Number of records:                                 98654\n",
      "Number of visits:                                  98654\n",
      "Number of webpages with no associated visitor ID:  0\n",
      "Number of visitors (IDs):                          32711\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x EDA_HW52.sh\n",
    "!./EDA_HW52.sh InnerJoinTable.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course (being this an inner join), all webpage URLs are matched with visitor IDs and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000,/regwiz,10001\r\n",
      "1000,/regwiz,10010\r\n",
      "1000,/regwiz,10039\r\n",
      "1000,/regwiz,10073\r\n",
      "1000,/regwiz,10087\r\n",
      "1000,/regwiz,10101\r\n",
      "1000,/regwiz,10132\r\n",
      "1000,/regwiz,10141\r\n",
      "1000,/regwiz,10154\r\n",
      "1000,/regwiz,10162\r\n",
      "1000,/regwiz,10166\r\n",
      "1000,/regwiz,10201\r\n",
      "1000,/regwiz,10218\r\n",
      "1000,/regwiz,10220\r\n",
      "1000,/regwiz,10324\r\n",
      "1000,/regwiz,10348\r\n",
      "1000,/regwiz,10376\r\n",
      "1000,/regwiz,10384\r\n",
      "1000,/regwiz,10409\r\n",
      "1000,/regwiz,10429\r\n",
      "1000,/regwiz,10454\r\n",
      "1000,/regwiz,10457\r\n",
      "1000,/regwiz,10471\r\n",
      "1000,/regwiz,10497\r\n",
      "1000,/regwiz,10511\r\n",
      "1000,/regwiz,10520\r\n",
      "1000,/regwiz,10541\r\n",
      "1000,/regwiz,10564\r\n",
      "1000,/regwiz,10599\r\n",
      "1000,/regwiz,10752\r\n",
      "1000,/regwiz,10756\r\n",
      "1000,/regwiz,10861\r\n",
      "1000,/regwiz,10935\r\n",
      "1000,/regwiz,10943\r\n",
      "1000,/regwiz,10969\r\n",
      "1000,/regwiz,11027\r\n",
      "1000,/regwiz,11050\r\n",
      "1000,/regwiz,11410\r\n",
      "1000,/regwiz,11429\r\n",
      "1000,/regwiz,11440\r\n",
      "1000,/regwiz,11490\r\n",
      "1000,/regwiz,11501\r\n",
      "1000,/regwiz,11528\r\n",
      "1000,/regwiz,11539\r\n",
      "1000,/regwiz,11544\r\n",
      "1000,/regwiz,11685\r\n",
      "1000,/regwiz,11695\r\n",
      "1000,/regwiz,11723\r\n",
      "1000,/regwiz,11766\r\n",
      "1000,/regwiz,11774\r\n"
     ]
    }
   ],
   "source": [
    "!head -50 InnerJoinTable.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.2.2: Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MRJob task for Right Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HashSideRightJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HashSideRightJoin.py\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "    \n",
    "class HashSideRightJoin(MRJob):\n",
    "\n",
    "    def mapper_init(self):\n",
    "        # Load left-side table in memory as dictionary\n",
    "        self.TL = {}\n",
    "        # The absolute path will be passed as argument when calling MRJob\n",
    "        for key, value in csv.reader(open(\"TableLeft.txt\", \"r\")):\n",
    "            # key = webpage ID, value = webpage URL\n",
    "            self.TL[key] = value   \n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        # Iterate over the right-side table, a record at a time\n",
    "        TRrecord = line.split(\",\")\n",
    "        key = TRrecord[0]\n",
    "        value_visitor = TRrecord[1]\n",
    "        # Look for each record, in the left-side table (in-memory)\n",
    "        if key in self.TL.keys():\n",
    "            yield key, (self.TL[key], value_visitor)\n",
    "        # And if there's no match, include the visitor info anyway\n",
    "        else:\n",
    "            yield key, (None, value_visitor)\n",
    "    \n",
    "    # The reducer is optional. If not specified, I found out records are not \n",
    "        # sorted by webpage ID\n",
    "    def reducer(self, key, value):\n",
    "        for val_url, val_visitor in value:\n",
    "            yield key, (val_url, val_visitor)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    HashSideRightJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Call Python Script with Right Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python HW52.py Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###EDA and output of the right join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory analysis of the joint table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of webpage IDs:                             285\n",
      "Number of webpage URLs:                            285\n",
      "Number of webpages visited:                        285\n",
      "Number of webpages with no associated webpage URL: 0\n",
      "Number of records:                                 98654\n",
      "Number of visits:                                  98654\n",
      "Number of webpages with no associated visitor ID:  0\n",
      "Number of visitors (IDs):                          32711\n"
     ]
    }
   ],
   "source": [
    "!./EDA_HW52.sh RightJoinTable.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being this a right join, we might have found some visits not matched with any URL, but that's not the case (because all primary keys: the webpage IDs) appear in the left-side table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000,/regwiz,10001\r\n",
      "1000,/regwiz,10010\r\n",
      "1000,/regwiz,10039\r\n",
      "1000,/regwiz,10073\r\n",
      "1000,/regwiz,10087\r\n",
      "1000,/regwiz,10101\r\n",
      "1000,/regwiz,10132\r\n",
      "1000,/regwiz,10141\r\n",
      "1000,/regwiz,10154\r\n",
      "1000,/regwiz,10162\r\n",
      "1000,/regwiz,10166\r\n",
      "1000,/regwiz,10201\r\n",
      "1000,/regwiz,10218\r\n",
      "1000,/regwiz,10220\r\n",
      "1000,/regwiz,10324\r\n",
      "1000,/regwiz,10348\r\n",
      "1000,/regwiz,10376\r\n",
      "1000,/regwiz,10384\r\n",
      "1000,/regwiz,10409\r\n",
      "1000,/regwiz,10429\r\n",
      "1000,/regwiz,10454\r\n",
      "1000,/regwiz,10457\r\n",
      "1000,/regwiz,10471\r\n",
      "1000,/regwiz,10497\r\n",
      "1000,/regwiz,10511\r\n",
      "1000,/regwiz,10520\r\n",
      "1000,/regwiz,10541\r\n",
      "1000,/regwiz,10564\r\n",
      "1000,/regwiz,10599\r\n",
      "1000,/regwiz,10752\r\n",
      "1000,/regwiz,10756\r\n",
      "1000,/regwiz,10861\r\n",
      "1000,/regwiz,10935\r\n",
      "1000,/regwiz,10943\r\n",
      "1000,/regwiz,10969\r\n",
      "1000,/regwiz,11027\r\n",
      "1000,/regwiz,11050\r\n",
      "1000,/regwiz,11410\r\n",
      "1000,/regwiz,11429\r\n",
      "1000,/regwiz,11440\r\n",
      "1000,/regwiz,11490\r\n",
      "1000,/regwiz,11501\r\n",
      "1000,/regwiz,11528\r\n",
      "1000,/regwiz,11539\r\n",
      "1000,/regwiz,11544\r\n",
      "1000,/regwiz,11685\r\n",
      "1000,/regwiz,11695\r\n",
      "1000,/regwiz,11723\r\n",
      "1000,/regwiz,11766\r\n",
      "1000,/regwiz,11774\r\n"
     ]
    }
   ],
   "source": [
    "!head -50 RightJoinTable.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.2.3: Left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MRJob task for Left Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HashSideLeftJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HashSideLeftJoin.py\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "    \n",
    "class HashSideLeftJoin(MRJob):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(HashSideLeftJoin, self).__init__(*args, **kwargs)\n",
    "        self.TLkeys = []\n",
    "\n",
    "    def mapper_init(self):\n",
    "        # Load left-side table in memory as dictionary\n",
    "        self.TL = {}\n",
    "        # The absolute path will be passed as argument when calling MRJob\n",
    "        for key, value in csv.reader(open(\"TableLeft.txt\", \"r\")):\n",
    "            # key = webpage ID, value = webpage URL\n",
    "            self.TL[key] = value   \n",
    "            self.TLkeys.append(key)\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        # Iterate over the right-side table, a record at a time\n",
    "        TRrecord = line.split(\",\")\n",
    "        key = TRrecord[0]\n",
    "        value_visitor = TRrecord[1]\n",
    "        # Look for each record, in the left-side table (in-memory)\n",
    "        if key in self.TL.keys():\n",
    "            try:\n",
    "                self.TLkeys.remove(key)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            yield key, (self.TL[key], value_visitor)\n",
    "    \n",
    "    def mapper_final(self):\n",
    "        # Iterate over the right-side table, a record at a time\n",
    "        for key in self.TLkeys:\n",
    "            yield key, (self.TL[key], None)\n",
    "    \n",
    "    def reducer(self, key, value):\n",
    "        for val_url, val_visitor in value:\n",
    "            yield key, (val_url, val_visitor)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    HashSideLeftJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Call Python Script with Left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python HW52.py Left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###EDA and output of the left join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory analysis of the joint table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of webpage IDs:                             294\n",
      "Number of webpage URLs:                            294\n",
      "Number of webpages visited:                        285\n",
      "Number of webpages with no associated webpage URL: 0\n",
      "Number of records:                                 98663\n",
      "Number of visits:                                  98654\n",
      "Number of webpages with no associated visitor ID:  9\n",
      "Number of visitors (IDs):                          32711\n",
      "Webpages with no visits or URL:\n",
      "\t1287,/autoroute,None\n",
      "\t1288,/library,None\n",
      "\t1289,/masterchef,None\n",
      "\t1290,/devmovies,None\n",
      "\t1291,/news,None\n",
      "\t1292,/northafrica,None\n",
      "\t1293,/encarta,None\n",
      "\t1294,/bookshelf,None\n",
      "\t1297,/centroam,None\n"
     ]
    }
   ],
   "source": [
    "!./EDA_HW52.sh LeftJoinTable.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this joint table contains 9 more records than the other two, since 9 URLs are not matched with any visitor IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000,/regwiz,10001\r\n",
      "1000,/regwiz,10010\r\n",
      "1000,/regwiz,10039\r\n",
      "1000,/regwiz,10073\r\n",
      "1000,/regwiz,10087\r\n",
      "1000,/regwiz,10101\r\n",
      "1000,/regwiz,10132\r\n",
      "1000,/regwiz,10141\r\n",
      "1000,/regwiz,10154\r\n",
      "1000,/regwiz,10162\r\n",
      "1000,/regwiz,10166\r\n",
      "1000,/regwiz,10201\r\n",
      "1000,/regwiz,10218\r\n",
      "1000,/regwiz,10220\r\n",
      "1000,/regwiz,10324\r\n",
      "1000,/regwiz,10348\r\n",
      "1000,/regwiz,10376\r\n",
      "1000,/regwiz,10384\r\n",
      "1000,/regwiz,10409\r\n",
      "1000,/regwiz,10429\r\n",
      "1000,/regwiz,10454\r\n",
      "1000,/regwiz,10457\r\n",
      "1000,/regwiz,10471\r\n",
      "1000,/regwiz,10497\r\n",
      "1000,/regwiz,10511\r\n",
      "1000,/regwiz,10520\r\n",
      "1000,/regwiz,10541\r\n",
      "1000,/regwiz,10564\r\n",
      "1000,/regwiz,10599\r\n",
      "1000,/regwiz,10752\r\n",
      "1000,/regwiz,10756\r\n",
      "1000,/regwiz,10861\r\n",
      "1000,/regwiz,10935\r\n",
      "1000,/regwiz,10943\r\n",
      "1000,/regwiz,10969\r\n",
      "1000,/regwiz,11027\r\n",
      "1000,/regwiz,11050\r\n",
      "1000,/regwiz,11410\r\n",
      "1000,/regwiz,11429\r\n",
      "1000,/regwiz,11440\r\n",
      "1000,/regwiz,11490\r\n",
      "1000,/regwiz,11501\r\n",
      "1000,/regwiz,11528\r\n",
      "1000,/regwiz,11539\r\n",
      "1000,/regwiz,11544\r\n",
      "1000,/regwiz,11685\r\n",
      "1000,/regwiz,11695\r\n",
      "1000,/regwiz,11723\r\n",
      "1000,/regwiz,11766\r\n",
      "1000,/regwiz,11774\r\n"
     ]
    }
   ],
   "source": [
    "!head -50 LeftJoinTable.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the remainder of this assignment you will work with a large subset of the Google n-grams dataset, [https://aws.amazon.com/datasets/google-books-ngrams/](https://aws.amazon.com/datasets/google-books-ngrams/), which we have placed in a bucket on s3: [s3://filtered-5grams/](s3://filtered-5grams/).**\n",
    "\n",
    "**In particular, this bucket contains (~200) files in the format:**\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "**Do some EDA on this dataset using mrjob, e.g., **\n",
    "\n",
    "- **Longest 5-gram (number of characters)**\n",
    "- **Top 10 most frequent words (count), i.e., unigrams**\n",
    "- **Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)**\n",
    "- **Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)**\n",
    "\n",
    "**OPTIONAL Question:**\n",
    "\n",
    "- **Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?**\n",
    "\n",
    "**For more background see:**\n",
    "\n",
    "**[https://en.wikipedia.org/wiki/Log%E2%80%93log_plot](https://en.wikipedia.org/wiki/Log%E2%80%93log_plot)**\n",
    "\n",
    "**[https://en.wikipedia.org/wiki/Power_law](https://en.wikipedia.org/wiki/Power_law)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: aws [options] <command> <subcommand> [parameters]\r\n",
      "aws: error: argument command: Invalid choice, valid choices are:\r\n",
      "\r\n",
      "autoscaling                              | cloudformation                          \r\n",
      "cloudfront                               | cloudhsm                                \r\n",
      "cloudsearch                              | cloudsearchdomain                       \r\n",
      "cloudtrail                               | cloudwatch                              \r\n",
      "codecommit                               | codepipeline                            \r\n",
      "cognito-identity                         | cognito-sync                            \r\n",
      "datapipeline                             | devicefarm                              \r\n",
      "directconnect                            | ds                                      \r\n",
      "dynamodb                                 | dynamodbstreams                         \r\n",
      "ec2                                      | ecs                                     \r\n",
      "efs                                      | elasticache                             \r\n",
      "elasticbeanstalk                         | elastictranscoder                       \r\n",
      "elb                                      | emr                                     \r\n",
      "glacier                                  | iam                                     \r\n",
      "importexport                             | kinesis                                 \r\n",
      "kms                                      | lambda                                  \r\n",
      "logs                                     | machinelearning                         \r\n",
      "opsworks                                 | rds                                     \r\n",
      "redshift                                 | route53                                 \r\n",
      "route53domains                           | sdb                                     \r\n",
      "ses                                      | sns                                     \r\n",
      "sqs                                      | ssm                                     \r\n",
      "storagegateway                           | sts                                     \r\n",
      "support                                  | swf                                     \r\n",
      "workspaces                               | s3api                                   \r\n",
      "s3                                       | configure                               \r\n",
      "deploy                                   | configservice                           \r\n",
      "help                                    \r\n"
     ]
    }
   ],
   "source": [
    "!aws version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: s3://ucb-mids-mls-juanjocarin/\n",
      "upload: ./gbooks_filtered_sample.txt to s3://ucb-mids-mls-hw5/gbooks_filtered_sample.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mb s3://ucb-mids-mls-juanjocarin/\n",
    "!aws s3 cp gbooks_filtered_sample.txt \\\n",
    "    s3://ucb-mids-mls-hw5/gbooks_filtered_sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.3.1: Longest 5-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have to aggregate the lengths of each n-gram, just pass them to the reducer, we can use them as keys and sort by the length (from highest to lowest).\n",
    "\n",
    "Apart from that, to reduce the traffic between the mappers and the reducer, as well as the size of the output file, their outputs are only the longest n-gram they receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Longest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Longest.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "class Longest(MRJob):\n",
    "    \n",
    "    # Define a global variable that captures the longest n-gram found 'til now\n",
    "    longest = 0 \n",
    "    \n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(Longest, self).jobconf()        \n",
    "        custom_jobconf = {\n",
    "            'mapred.output.key.comparator.class': \n",
    "                'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1rn',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper = self.mapper, reducer = self.reducer)]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        [ngram,count,pages,books] = re.split(\"\\t\",line)\n",
    "        length = len(ngram)-4 # Lenght of the n-gram excluding (n-1) spaces\n",
    "        # Only yield results if current n-gram is equal or longer than previous \n",
    "            # ones\n",
    "        # This part is optional, but dramatically reduces the mappers' outputs\n",
    "        if length >= self.longest:\n",
    "            self.longest = length\n",
    "            yield int(length),ngram\n",
    "    \n",
    "    def reducer(self,length,values):\n",
    "        # Again, compare with previous n-grams\n",
    "        if int(length) >= self.longest:\n",
    "            self.longest = int(length)\n",
    "            for ngram in values:\n",
    "                yield length, ngram\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Longest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x Longest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Longest_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Longest_driver.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from Longest import Longest\n",
    "mr_job = Longest(args=\n",
    "                 ['s3://filtered-5grams/',\n",
    "                  '-r', 'emr'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        length,ngram = mr_job.parse_output_line(line)\n",
    "        print str(length) + \"\\t\" + ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x Longest_driver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\tROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\r\n",
      "155\tAIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\r\n"
     ]
    }
   ],
   "source": [
    "!python Longest_driver.py\n",
    "# No need to print first values: there is only one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.3.2: Top 10 most frequent words (unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to add the number of times a unigram occurs, for all the 5-grams that include that unigram, so we cannot use count as the key. To reduce the traffic load between the mappers and the reducer, we can use combiners. And in the final step, keep a dictionary of 10 items (10 arbitrary keys whose values are all zero), that is updated with the most frequent unigrams.\n",
    "\n",
    "Hence, the output of the reducer will only contain the top 10 most frequent words. Again, we just have to print the whole output; in this case we must also sort it, but that won't consume many resources (it contains only 10 elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting FreqUnigrams.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FreqUnigrams.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "class FreqUnigrams(MRJob):\n",
    "     \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper = self.mapper, combiner = self.combiner, \n",
    "                       reducer_init = self.reducer_init, \n",
    "                       reducer = self.reducer, \n",
    "                       reducer_final = self.reducer_final)]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        [ngram,count,pages,books] = re.split(\"\\t\",line)\n",
    "        # Output the count for each word in the 5-gram\n",
    "        for unigram in ngram.split():\n",
    "            yield unigram,int(count)\n",
    "\n",
    "    # Aggregate partial results before passing to reducer\n",
    "    def combiner(self, unigram, count):\n",
    "        partial = sum(c for c in count)\n",
    "        yield unigram,int(partial)\n",
    "            \n",
    "    # Initialize a dictionary with top10 (initially 10 arbitrary keys \n",
    "        # whose value is 0)\n",
    "    def reducer_init(self):\n",
    "        self.top = {}\n",
    "        import string\n",
    "        for i in string.lowercase[:10]:\n",
    "            self.top[i]=0\n",
    "\n",
    "    def reducer(self,unigram,partial):\n",
    "        # Aggregate counts\n",
    "        total = sum(p for p in partial)\n",
    "        # If higher than what's already in the Top10...\n",
    "        if total > min(self.top.values()):\n",
    "            # remove minimum value\n",
    "            self.top.pop(min(self.top, key=self.top.get))\n",
    "            # Substitute with new one\n",
    "            self.top[unigram] = total\n",
    "\n",
    "    # Output only Top10\n",
    "    def reducer_final(self):\n",
    "        for k,v in self.top.iteritems():\n",
    "            yield v,k\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    FreqUnigrams.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x FreqUnigrams.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting FreqUnigrams_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FreqUnigrams_driver.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from FreqUnigrams import FreqUnigrams\n",
    "mr_job = FreqUnigrams(args=\n",
    "                 ['s3://filtered-5grams/',\n",
    "                  '-r', 'emr'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        freq,unigram = mr_job.parse_output_line(line)\n",
    "        print str(freq) + \"\\t\" + unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x FreqUnigrams_driver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5375699242\tthe\r\n",
      "3691308874\tof\r\n",
      "2221164346\tto\r\n",
      "1387638591\tin\r\n",
      "1342195425\ta\r\n",
      "1135779433\tand\r\n",
      "798553959\tthat\r\n",
      "756296656\tis\r\n",
      "688053106\tbe\r\n",
      "481373389\tas\r\n"
     ]
    }
   ],
   "source": [
    "!./FreqUnigrams_driver.py | sort -rn # Sort the 10 items in reverse order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.3.3: Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Density.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Density.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "class Density(MRJob):\n",
    "     \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper = self.mapper, combiner = self.combiner, \n",
    "                       reducer = self.reducer)]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        [ngram,count,pages,books] = re.split(\"\\t\",line)\n",
    "        # Output the count for each word in the 5-gram\n",
    "        for unigram in ngram.split():\n",
    "            # Value: count & pages\n",
    "            yield unigram,[int(count),int(pages)]\n",
    "\n",
    "    # Aggregate partial results before passing to reducer\n",
    "    def combiner(self, unigram, duple):\n",
    "        partial_count = 0\n",
    "        partial_pages = 0\n",
    "        for count,pages in duple:\n",
    "            partial_count += count\n",
    "            partial_pages += pages\n",
    "        yield unigram,(int(partial_count),int(partial_pages))\n",
    "\n",
    "    def reducer(self,unigram,duple):\n",
    "        # Aggregate results\n",
    "        total_count = 0\n",
    "        total_pages = 0\n",
    "        for count,pages in duple:\n",
    "            total_count += count\n",
    "            total_pages += pages\n",
    "        # Calculate density (minimum value will be 1.0)\n",
    "        density = float(total_count)/total_pages\n",
    "        yield density,unigram\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Density.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x Density.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Density_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Density_driver.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from Density import Density\n",
    "import os\n",
    "mr_job = Density(args=[\n",
    "        's3://filtered-5grams/','-r', 'emr',\n",
    "        '--output-dir=s3://ucb-mids-mls-juanjocarin/Density_output',\n",
    "        '--no-output'\n",
    "    ])\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "os.system(\"aws s3 cp s3://ucb-mids-mls-juanjocarin/Density_output/part-00000 \\\n",
    "    s3://ucb-mids-mls-juanjocarin/DenseUnigrams.txt\")\n",
    "os.system(\"aws s3 rm s3://ucb-mids-mls-juanjocarin/Density_output/part-00000\")\n",
    "os.system(\"aws s3 rm s3://ucb-mids-mls-juanjocarin/Density_output/_SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x Density_driver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://ucb-mids-mls-juanjocarin/Density_output/part-00000 to s3://ucb-mids-mls-juanjocarin/DenseUnigrams.txt\n",
      "delete: s3://ucb-mids-mls-juanjocarin/Density_output/part-00000\n",
      "delete: s3://ucb-mids-mls-juanjocarin/Density_output/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!./Density_driver.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-juanjocarin/DenseUnigrams.txt to ./DenseUnigrams.txt\n",
      "delete: s3://ucb-mids-mls-juanjocarin/DenseUnigrams.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://ucb-mids-mls-juanjocarin/DenseUnigrams.txt ./DenseUnigrams.txt\n",
    "!aws s3 rm s3://ucb-mids-mls-juanjocarin/DenseUnigrams.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Most densely appearing words sorted in decreasing order of relative frequency (count/pages_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.557291666666666\t\"xxxx\"\r\n",
      "10.161726044782885\t\"NA\"\r\n",
      "8.0741599073001158\t\"blah\"\r\n",
      "7.5333333333333332\t\"nnn\"\r\n",
      "6.5611436445056839\t\"nd\"\r\n",
      "5.4073642846747196\t\"ND\"\r\n",
      "4.921875\t\"oooooooooooooooo\"\r\n",
      "4.7272727272727275\t\"PIC\"\r\n",
      "4.5116279069767442\t\"llll\"\r\n",
      "4.3494983277591972\t\"LUTHER\"\r\n",
      "4.2072378595731514\t\"oooooo\"\r\n",
      "4.0908402725208175\t\"NN\"\r\n",
      "3.9492846924177396\t\"ooooo\"\r\n",
      "3.9313725490196076\t\"OOOOOO\"\r\n",
      "3.7877030162412995\t\"IIII\"\r\n",
      "3.7624521072796937\t\"lillelu\"\r\n",
      "3.6570701447431206\t\"OOOOO\"\r\n",
      "3.6065624999999999\t\"Sc\"\r\n",
      "3.5769230769230771\t\"Pfeffermann\"\r\n",
      "3.5769230769230771\t\"Madarassy\"\r\n",
      "3.5600000000000001\t\"Meteoritical\"\r\n",
      "3.5364916773367479\t\"Undecided\"\r\n",
      "3.505639097744361\t\"Lib\"\r\n",
      "3.5\t\"xxxxxxxx\"\r\n",
      "3.4791318864774623\t\"ri\"\r\n",
      "3.3750684931506849\t\"Vir\"\r\n",
      "3.2390171258376768\t\"DREAM\"\r\n",
      "3.2290388548057258\t\"beep\"\r\n",
      "3.1886792452830188\t\"Latha\"\r\n",
      "3.1883175058233291\t\"MARTIN\"\r\n",
      "3.1699346405228757\t\"Lis\"\r\n",
      "3.1147458480120784\t\"Ac\"\r\n",
      "3.0371428571428569\t\"OUTPUT\"\r\n",
      "3.0222222222222221\t\"HENNESSY\"\r\n",
      "3.0\t\"ALLIS\"\r\n",
      "2.9191176470588234\t\"IYENGAR\"\r\n",
      "2.8698912704670052\t\"ft\"\r\n",
      "2.8432451923076925\t\"Adapted\"\r\n",
      "2.8250000000000002\t\"counterfeiteth\"\r\n",
      "2.8198198198198199\t\"nonsquamous\"\r\n",
      "2.8198198198198199\t\"nonmorular\"\r\n",
      "2.8085106382978724\t\"RHYME\"\r\n",
      "2.7446808510638299\t\"YOUTHS\"\r\n",
      "2.7264957264957266\t\"Poing\"\r\n",
      "2.7000000000000002\t\"Kuhl\"\r\n",
      "2.6748466257668713\t\"Sirignano\"\r\n",
      "2.6734693877551021\t\"YARDS\"\r\n",
      "2.6734693877551021\t\"METRES\"\r\n",
      "2.66414686825054\t\"Illl\"\r\n",
      "2.6603773584905661\t\"Neophytos\"\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!sort -rn DenseUnigrams.txt | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Least densely appearing words sorted in decreasing order of relative frequency (count/pages_count)\n",
    "\n",
    "It's not worth printing many of them, since almost 200K have a relative frequency of exactly 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams that never appear more than once in a page: 166114\n",
      "\n",
      "1.0\t\"Aana\"\n",
      "1.0\t\"AAN\"\n",
      "1.0\t\"Aan\"\n",
      "1.0\t\"aame\"\n",
      "1.0\t\"AAMC\"\n",
      "1.0\t\"Aaltonen\"\n",
      "1.0\t\"AAL\"\n",
      "1.0\t\"aahs\"\n",
      "1.0\t\"AAHPERD\"\n",
      "1.0\t\"aahed\"\n",
      "1.0\t\"aah\"\n",
      "1.0\t\"Aagje\"\n",
      "1.0\t\"AAFES\"\n",
      "1.0\t\"AAE\"\n",
      "1.0\t\"Aadam\"\n",
      "1.0\t\"AACVPR\"\n",
      "1.0\t\"AACP\"\n",
      "1.0\t\"AAAE\"\n",
      "1.0\t\"AAAA\"\n",
      "1.0\t\"aA\"\n"
     ]
    }
   ],
   "source": [
    "!echo -e \"Number of unigrams that never appear more than once in a page: \"\\\n",
    "    $(grep $'1.0\\t' DenseUnigrams.txt | wc -l)\"\\n\"\n",
    "!sort -rn DenseUnigrams.txt | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. To accomplish this you must script two main tasks using MRJob:**\n",
    "\n",
    "1. **Build stripes of word co-ocurrence for the top 10,000 most frequently appearing words across the entire set of 5-grams, and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).**\n",
    "\n",
    "2. **Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare  all stripes (vectors), and output to a file in your bucket on s3.**\n",
    "\n",
    "> Design notes for (1)\n",
    "\n",
    "> For this task you will be able to modify the pattern we used in HW 3.2 (feel free to use the solution as reference). To total the word counts across the 5-grams, output the support from the mappers using the total order inversion pattern:\n",
    "\n",
    "    > <*word,count>\n",
    "\n",
    "> to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "> In addition to ensuring the determination of the total word counts, the mapper must also output co-occurrence counts for the pairs of words inside of each 5-gram. Treat these words as a basket, as we have in HW 3, but count all stripes or pairs in both orders, i.e., count both orderings: (word1,word2), and (word2,word1), to preserve symmetry in our output for (2).\n",
    "\n",
    "> Design notes for (2)\n",
    "\n",
    "> For this task you will have to determine a method of comparison. Here are a few that you might consider:\n",
    "\n",
    "> - Spearman correlation\n",
    "> - Euclidean distance\n",
    "> - Taxicab (Manhattan) distance\n",
    "> - Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "> - Pearson correlation\n",
    "> - Cosine similarity\n",
    "> - Kendall correlation\n",
    "> - ...\n",
    "\n",
    "> However, be cautioned that some comparison methods are more difficult to parallelize than others, and do not perform more associations than is necessary, since your choice of association will be symmetric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've chosen **Manhattan** (or **Taxicab**) **distance** (Euclideand distance could be implemented in the same way just calculating the square of all differences, and the square root of the sum of those squared differences), which is defined as:\n",
    "\n",
    "$$d_1(\\mathbf{p},\\mathbf{q})=\\left \\|\\mathbf{p}-\\mathbf{q}  \\right \\|_1=\n",
    "\\sum_{i=1}^N\\left | p_i-q_i \\right |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW541_TopN.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW541_TopN.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class HW541_TopN(MRJob):\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(HW541_TopN, self).configure_options()\n",
    "        # The number of most frequent unigrams can be configured by\n",
    "            # the user as an argument\n",
    "        self.add_passthrough_option('--number_unigrams',  \n",
    "                                    dest='number_unigrams', type='int', \n",
    "                                    default=10)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper = self.mapper, combiner = self.combiner,\n",
    "                       reducer_init = self.reducer_init, \n",
    "                       reducer = self.reducer, \n",
    "                       reducer_final = self.reducer_final)]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        [ngram,count,pages,books] = re.split(\"\\t\",line)\n",
    "        # Output the count for each word in the 5-gram\n",
    "        unigrams = ngram.split()\n",
    "        for unigram in unigrams:\n",
    "            yield unigram, int(count)\n",
    "\n",
    "    def combiner(self, unigram, count):\n",
    "        yield unigram, sum(count)\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.top = {}\n",
    "\n",
    "    def reducer(self, unigram, count):\n",
    "        total = sum(count)\n",
    "        # If we have not exceeded max size of the dictionary yet\n",
    "        if len(self.top.keys()) < self.options.number_unigrams:\n",
    "            self.top[unigram] = total\n",
    "        # If exceeded, include new unigram only if more frequent that\n",
    "                # other previously stored\n",
    "        else:\n",
    "            if total > min(self.top.values()):\n",
    "                # Remove unigram not so frequent\n",
    "                self.top.pop(min(self.top, key = self.top.get))\n",
    "                # Add new unigram\n",
    "                self.top[unigram] = total\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        for unigram in self.top.keys():\n",
    "            yield unigram, self.top[unigram]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    HW541_TopN.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x HW541_TopN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/hduser/.mrjob.conf\n",
      "using existing scratch bucket mrjob-03e94e1f06830625\n",
      "using s3://mrjob-03e94e1f06830625/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /tmp/HW541_TopN.hduser.20151008.185915.107662\n",
      "writing master bootstrap script to /tmp/HW541_TopN.hduser.20151008.185915.107662/b.py\n",
      "Copying non-input files into s3://mrjob-03e94e1f06830625/tmp/HW541_TopN.hduser.20151008.185915.107662/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-F8DJDNV76J81\n",
      "Created new job flow j-F8DJDNV76J81\n",
      "Job launched 31.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 93.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 186.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 217.3s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 248.3s ago, status RUNNING: Running step (HW541_TopN.hduser.20151008.185915.107662: Step 1 of 1)\n",
      "Job launched 279.4s ago, status RUNNING: Running step (HW541_TopN.hduser.20151008.185915.107662: Step 1 of 1)\n",
      "Job launched 310.4s ago, status RUNNING: Running step (HW541_TopN.hduser.20151008.185915.107662: Step 1 of 1)\n",
      "Job launched 341.8s ago, status RUNNING: Running step (HW541_TopN.hduser.20151008.185915.107662: Step 1 of 1)\n",
      "Job launched 373.0s ago, status RUNNING: Running step (HW541_TopN.hduser.20151008.185915.107662: Step 1 of 1)\n",
      "Job launched 404.1s ago, status RUNNING: Running step (HW541_TopN.hduser.20151008.185915.107662: Step 1 of 1)\n",
      "Job launched 435.2s ago, status RUNNING: Running step (HW541_TopN.hduser.20151008.185915.107662: Step 1 of 1)\n"
     ]
    }
   ],
   "source": [
    "!./HW541_TopN.py s3://filtered-5grams/ -r emr --number_unigrams=100 > Top.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the words, not their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"all\"\r\n",
      "\"think\"\r\n",
      "\"His\"\r\n",
      "\"had\"\r\n",
      "\"him\"\r\n",
      "\"to\"\r\n",
      "\"must\"\r\n",
      "\"Although\"\r\n",
      "\"struck\"\r\n",
      "\"has\"\r\n",
      "\"ought\"\r\n",
      "\"do\"\r\n",
      "\"his\"\r\n",
      "\"me\"\r\n",
      "\"were\"\r\n",
      "\"know\"\r\n",
      "\"they\"\r\n",
      "\"not\"\r\n",
      "\"he\"\r\n",
      "\"this\"\r\n",
      "\"told\"\r\n",
      "\"From\"\r\n",
      "\"For\"\r\n",
      "\"see\"\r\n",
      "\"been\"\r\n",
      "\"are\"\r\n",
      "\"what\"\r\n",
      "\"for\"\r\n",
      "\"may\"\r\n",
      "\"He\"\r\n",
      "\"be\"\r\n",
      "\"we\"\r\n",
      "\"University\"\r\n",
      "\"never\"\r\n",
      "\"by\"\r\n",
      "\"on\"\r\n",
      "\"her\"\r\n",
      "\"could\"\r\n",
      "\"place\"\r\n",
      "\"or\"\r\n",
      "\"first\"\r\n",
      "\"Even\"\r\n",
      "\"one\"\r\n",
      "\"long\"\r\n",
      "\"should\"\r\n",
      "\"your\"\r\n",
      "\"from\"\r\n",
      "\"would\"\r\n",
      "\"there\"\r\n",
      "\"But\"\r\n",
      "\"doubt\"\r\n",
      "\"certain\"\r\n",
      "\"meeting\"\r\n",
      "\"that\"\r\n",
      "\"took\"\r\n",
      "\"American\"\r\n",
      "\"with\"\r\n",
      "\"History\"\r\n",
      "\"And\"\r\n",
      "\"myself\"\r\n",
      "\"these\"\r\n",
      "\"was\"\r\n",
      "\"will\"\r\n",
      "\"can\"\r\n",
      "\"of\"\r\n",
      "\"my\"\r\n",
      "\"and\"\r\n",
      "\"Court\"\r\n",
      "\"give\"\r\n",
      "\"God\"\r\n",
      "\"is\"\r\n",
      "\"am\"\r\n",
      "\"it\"\r\n",
      "\"an\"\r\n",
      "\"How\"\r\n",
      "\"as\"\r\n",
      "\"at\"\r\n",
      "\"have\"\r\n",
      "\"in\"\r\n",
      "\"seen\"\r\n",
      "\"if\"\r\n",
      "\"no\"\r\n",
      "\"After\"\r\n",
      "\"when\"\r\n",
      "\"also\"\r\n",
      "\"you\"\r\n",
      "\"A\"\r\n",
      "\"shall\"\r\n",
      "\"I\"\r\n",
      "\"upon\"\r\n",
      "\"man\"\r\n",
      "\"a\"\r\n",
      "\"All\"\r\n",
      "\"An\"\r\n",
      "\"thought\"\r\n",
      "\"As\"\r\n",
      "\"so\"\r\n",
      "\"At\"\r\n",
      "\"time\"\r\n",
      "\"the\"\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f1 Top.txt > Top.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW541.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW541.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations \n",
    "from operator import itemgetter\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class HW541(MRJob):\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(HW541, self).configure_options()\n",
    "        # The number of most frequent unigrams can be configured by\n",
    "            # the user as an argument\n",
    "        self.add_passthrough_option('--number_unigrams',  \n",
    "                                    dest='number_unigrams', type='int', \n",
    "                                    default=20)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init, mapper = self.mapper, \n",
    "                       mapper_final = self.mapper_final, \n",
    "                       reducer_init = self.reducer_init, reducer = self.reducer, \n",
    "                       reducer_final = self.reducer_final)]\n",
    "\n",
    "    def mapper_init(self):\n",
    "        ## Initialize the co-occurrence counts hash array\n",
    "        self.countsM = {}\n",
    "        self.supportM = {} # Total count for each unigram\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        [ngram,count,pages,books] = re.split(\"\\t\",line)\n",
    "        # Output the count for each word in the 5-gram\n",
    "        unigrams = ngram.lower().split()\n",
    "        for unigram in unigrams:\n",
    "            # Add to dictionary\n",
    "            self.supportM.setdefault(unigram,0)\n",
    "            # Update value\n",
    "            self.supportM[unigram] += int(count)\n",
    "        # Get all of the 2-sets\n",
    "        combs = list(combinations(unigrams,2))\n",
    "        for combination in combs:\n",
    "            unigram1,unigram2 = sorted(combination)\n",
    "            # Add to dictionary\n",
    "            self.countsM.setdefault(unigram1,{})\n",
    "            # Initialize the other word of the bigram\n",
    "            self.countsM[unigram1].setdefault(unigram2,0)\n",
    "            # Add the support of unigram1 & unigram2\n",
    "            self.countsM[unigram1][unigram2] += int(count)\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        # Place a * in for support order inversion in sort\n",
    "        for unigram in self.supportM.keys():\n",
    "            yield \"*\"+unigram, str(self.supportM[unigram])\n",
    "        # Emit co-occurrence counts to stream\n",
    "        for unigram in self.countsM.keys():\n",
    "            yield unigram, self.countsM[unigram]\n",
    "            \n",
    "    def reducer_init(self):\n",
    "        # Initialize similar dictionaries\n",
    "        self.countsR = {}\n",
    "        self.supportR = {}\n",
    "\n",
    "    def reducer(self,unigram,stripe):\n",
    "        N = self.options.number_unigrams \n",
    "            # To keep final dict size bound to the required value\n",
    "        # Check to see if this is a support line\n",
    "        if re.match(\"\\*\",unigram):\n",
    "            unigram = unigram[1:]\n",
    "            count = 0\n",
    "            for s in stripe:\n",
    "                count += int(s)\n",
    "            # If we have not exceeded max size of the dictionary yet\n",
    "            if len(self.supportR.keys()) < N:\n",
    "                self.supportR[unigram] = count\n",
    "            # If exceeded, include new unigram only if more frequent that\n",
    "                # other previously stored\n",
    "            else:\n",
    "                if count > min(self.supportR.values()):\n",
    "                    # Remove unigram not so frequent\n",
    "                    self.supportR.pop(min(self.supportR, \n",
    "                        key = self.supportR.get))\n",
    "                    # Add new unigram\n",
    "                    self.supportR[unigram] = count\n",
    "        else: # if it's a stripe\n",
    "            unigram1 = unigram\n",
    "            # Add stripe only if it corresponds to a frequent unigram\n",
    "            if unigram1 in self.supportR.keys():\n",
    "                self.countsR[unigram1]={}\n",
    "                for s in stripe:\n",
    "                    for unigram2 in s.keys():\n",
    "                        # Include bigram only if both words are frequent \n",
    "                        if unigram2 in self.supportR.keys():\n",
    "                            self.countsR[unigram1].setdefault(unigram2,0)\n",
    "                            self.countsR[unigram1][unigram2] += \\\n",
    "                                int(s[unigram2])\n",
    "                    \n",
    "    # Now we have kind of a triangular matrix, but some values are missing\n",
    "        # (when there is no co-occurrence) --  we have to fullfill them\n",
    "        # with zero, as well as the upper right side of the matrix\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        for unigram1 in sorted(self.supportR.keys()):\n",
    "            # If not in the hash table (the way the mapper was built,\n",
    "                # the existence of self. supportR[A][B] made unnecessary the\n",
    "                # the existence of self.supportR[A][B])\n",
    "            if unigram1 not in self.countsR.keys():\n",
    "                # Add it\n",
    "                self.countsR[unigram1]={}\n",
    "        # Double loop to build the complete matrix and fulfill missing values\n",
    "        for unigram1 in sorted(self.supportR.keys()):\n",
    "            for unigram2 in sorted(self.supportR.keys()):\n",
    "                if unigram2 not in self.countsR[unigram1].keys():\n",
    "                    if unigram1 in self.countsR[unigram2].keys():                        \n",
    "                        self.countsR[unigram1][unigram2] = \\\n",
    "                            self.countsR[unigram2][unigram1]\n",
    "                    else:\n",
    "                        self.countsR[unigram1][unigram2] = 0\n",
    "        # OUTPUT: a row of the confidence matrix, for each unigram\n",
    "        for unigram1 in sorted(self.supportR.keys()):\n",
    "            # Calculate the cooccurrences of each bigram\n",
    "            yield unigram1,','.join([\n",
    "                    str(self.countsR[unigram1][unigram2]) for unigram2 in \\\n",
    "                        sorted(self.supportR.keys())])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    HW541.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x HW541.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a test with a sample file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/hduser/.mrjob.conf\n",
      "creating tmp directory /tmp/HW541.hduser.20151008.040959.243053\n",
      "writing to /tmp/HW541.hduser.20151008.040959.243053/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/HW541.hduser.20151008.040959.243053/step-0-mapper-sorted\n",
      "> sort /tmp/HW541.hduser.20151008.040959.243053/step-0-mapper_part-00000\n",
      "writing to /tmp/HW541.hduser.20151008.040959.243053/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /tmp/HW541.hduser.20151008.040959.243053/step-0-reducer_part-00000 -> /tmp/HW541.hduser.20151008.040959.243053/output/part-00000\n",
      "Streaming final output from /tmp/HW541.hduser.20151008.040959.243053/output\n",
      "removing tmp directory /tmp/HW541.hduser.20151008.040959.243053\n"
     ]
    }
   ],
   "source": [
    "!./HW541.py gbooks_filtered_sample.txt --number_unigrams=100 > hw541_sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t\"5042,896,717,2617,891,1016,550,1627,419,1623,10445,770,8903,108380,2272,1491,7070,1905,2224,990,1395,629,550,239,911,0,719,247,15903,5247,73,8427,1126,11904,2660,5500,28994,1503,441,6506,943,1683,889,38533,347,14592,14543,2337,1809,2314,7060,1075,314,104283,1763,420,1152,54,554,8678,52,2587,165017,3043,416,1393,0,382,524,372,745,107,665,540,367,43,5402,129225,2363,1324,397,710,1943,2454,3365,12092,0,95,0,287,11444,928,1136,435,766,803,8364,1213,3252,62\"\r\n",
      "\"about\"\t\"896,0,0,632,0,167,0,0,64,0,555,48,86,43,105,0,126,181,0,0,218,0,60,0,0,0,0,0,749,120,66,0,0,0,0,180,433,0,121,54,0,0,103,5303,0,0,374,3997,67,0,0,0,42,0,0,0,0,0,0,0,0,0,691,76,395,0,0,0,0,0,0,0,0,0,0,0,79,1421,88,0,0,106,1021,4094,262,735,0,0,0,0,4169,749,0,126,0,0,0,0,0,0\"\r\n",
      "\"according\"\t\"717,0,0,0,0,0,0,0,0,52,290,45,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,54,0,93,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8137,0,206,0,0,0,0,0,0,0,0,0,0,0,0,8792,0,0,0,0,195,0,0,10297,0,0,0,0,45,0,0,0,0,43,0,0,0,0\"\r\n",
      "\"after\"\t\"2617,632,0,0,229,0,0,0,0,43,325,0,0,115,0,670,0,44,0,0,0,0,0,142,0,0,244,0,213,213,0,0,0,1756,368,449,768,393,0,0,802,0,0,568,0,418,267,0,330,55,0,40,0,0,0,0,234,0,0,0,75,0,5254,66,45,207,0,0,0,0,0,0,297,0,80,0,1442,7687,264,98,334,85,984,103,278,1248,0,97,142,0,485,1272,352,0,0,0,0,99,159,0\"\r\n",
      "\"all\"\t\"891,0,0,229,74,42,0,287,89,167,3870,776,167,1106,999,92,1199,268,146,0,906,0,268,0,559,0,95,96,957,1003,0,0,143,1071,510,1483,2699,57,0,0,931,0,43,3553,95,815,3975,1070,148,151,0,913,51,0,0,300,201,0,0,44,75,635,8891,353,371,99,0,0,0,0,0,73,57,60,111,0,2011,7302,174,1735,385,85,5563,50,190,1261,0,0,0,0,180,276,674,0,0,41,240,0,646,0\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 hw541_sample_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, $\\text{confidence}(\\text{unigram}_i,\\text{unigram}_i)=1.0 \\text{ } \\forall i \\in \\{1,N\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW541_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW541_driver.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from HW541 import HW541\n",
    "\n",
    "import os\n",
    "\n",
    "mr_job = HW541(args=['s3://filtered-5grams/', '-r', 'emr',\n",
    "                     '--number_unigrams=10000',\n",
    "                     '--output-dir=s3://ucb-mids-mls-juanjocarin/Confidence',\n",
    "                     '--no-output'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "os.system(\"aws s3 cp s3://ucb-mids-mls-juanjocarin/Confidence/part-00000 \\\n",
    "    s3://ucb-mids-mls-juanjocarin/Confidence.txt\")\n",
    "os.system(\"aws s3 rm s3://ucb-mids-mls-juanjocarin/Confidence/part-00000\")\n",
    "os.system(\"aws s3 rm s3://ucb-mids-mls-juanjocarin/Confidence/_SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x HW541_driver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!./HW541_driver.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW5.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that our coordinates are (for simplicity I'm using integers, though I've finally used confidences, which are float numbers):\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "7 & 8 & 5\\\\\n",
    "8 & 4 & 1\\\\ \n",
    "5 & 1 & 9\n",
    "\\end{pmatrix}$\n",
    "\n",
    "The (Manhattan) distance matrix is easy to calculate (and of course the elements in the diagonal will be null):\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "0 & 9 & 13\\\\\n",
    "9 & 0 & 14\\\\ \n",
    "13 & 14 & 0\n",
    "\\end{pmatrix}$\n",
    "\n",
    "With the first row, \n",
    "$\\begin{pmatrix}\n",
    "7 & 8 & 5\n",
    "\\end{pmatrix}$, corresponding to the 1st component, we can calculate $\\mid p_1 - q_1 \\mid$ for all possible combinations of $\\mathbf{p}$ and $\\mathbf{q}$: $\\begin{pmatrix}\n",
    "0 & 1 & 2\n",
    "\\end{pmatrix}$ if $\\mathbf{q}$ is the unigram corresponding to the first column, $\\begin{pmatrix}\n",
    "1 & 0 & 3\n",
    "\\end{pmatrix}$ if $\\mathbf{q}$ is the unigram corresponding to the second column, and so on. If we proceed the same way for all rows, for the unigram in the first column we could obtain the following matrix:\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "0 & 1 & 2\\\\\n",
    "0 & 4 & 7\\\\ \n",
    "0 & 4 & 4\n",
    "\\end{pmatrix}$\n",
    "\n",
    "The row-wise sum of this matrix corresponds to the first row in our distance matrix: $\\begin{pmatrix}\n",
    "0 & 9 & 13\n",
    "\\end{pmatrix}$, which give us the first component of the distance between the first unigram and itself, between that unigram and the second, and between the first unigram and the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW542.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW542.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations \n",
    "from operator import itemgetter\n",
    "from math import sqrt\n",
    "\n",
    "class HW542(MRJob):\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(HW542, self).configure_options()\n",
    "        # The number of most frequent unigrams can be configured by\n",
    "            # the user as an argument\n",
    "        self.add_passthrough_option('--number_unigrams',  \n",
    "                                    dest='number_unigrams', type='int', \n",
    "                                    default=20)\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper = self.mapper, \n",
    "                reducer = self.reducer)]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        # i-th line (corresponding to i-th unigram from the top 10,000 \n",
    "            # frequent contains the i-th coordinates for all unigrams\n",
    "        line = re.sub('\\\"', '', line)\n",
    "        line = line.split()\n",
    "        unigram = line[0]\n",
    "        coords = line[1].split(',')\n",
    "        # We have N (=10,000) coordinates and points\n",
    "        # For each row (or vector) of N elements we're going to calculate N\n",
    "            # other vectors, by subtracting the 1st, second, ... N-th element\n",
    "            # and taking the absolute value\n",
    "        # We also need the unigram, because (since they were ordered \n",
    "            # alphabetically) it will allow us to detect the value of \"i\"\n",
    "        partial_diff = []\n",
    "        for i in range(len(coords)):\n",
    "            partial_diff.append([abs(int(coords[i])-int(x)) for x in coords])\n",
    "        # Flatten to a single list\n",
    "        partial_diff = [item for sublist in partial_diff for item in sublist]\n",
    "        yield None, {unigram:partial_diff}\n",
    "    \n",
    "    def reducer(self, _, partial_diff):\n",
    "        N = self.options.number_unigrams # number of dimensions and unigrams\n",
    "        unigrams = []\n",
    "        distances = [0] * N * N\n",
    "        for p in partial_diff:\n",
    "            unigrams.append(p.keys()[0]) # just need 1st key, each dictionary\n",
    "                # only contains one, corresponding to one unigram\n",
    "            # Add distances row-wise\n",
    "            distances = [d+int(x) for d,x in zip(distances,p.values()[0])]\n",
    "        # Convert a single row (size N*N) to N rows of a NxN matrix\n",
    "        distance = []\n",
    "        i=0\n",
    "        while i<N*N:\n",
    "            distance.append(distances[i:i+N])\n",
    "            i += N\n",
    "        # 1st row contains the names of the unigrams\n",
    "        yield None, unigrams\n",
    "        # Subsequent N rows contains the distance to the other\n",
    "        for u,d in zip(sorted(unigrams),distance):\n",
    "            yield u,d\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    HW542.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x HW542.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue our test with the confidence matrix of the sample file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/hduser/.mrjob.conf\n",
      "creating tmp directory /tmp/HW542.hduser.20151008.041033.665064\n",
      "writing to /tmp/HW542.hduser.20151008.041033.665064/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/HW542.hduser.20151008.041033.665064/step-0-mapper-sorted\n",
      "> sort /tmp/HW542.hduser.20151008.041033.665064/step-0-mapper_part-00000\n",
      "writing to /tmp/HW542.hduser.20151008.041033.665064/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /tmp/HW542.hduser.20151008.041033.665064/step-0-reducer_part-00000 -> /tmp/HW542.hduser.20151008.041033.665064/output/part-00000\n",
      "Streaming final output from /tmp/HW542.hduser.20151008.041033.665064/output\n",
      "null\t[\"a\", \"about\", \"according\", \"after\", \"all\", \"also\", \"although\", \"am\", \"american\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"been\", \"but\", \"by\", \"can\", \"certain\", \"could\", \"court\", \"did\", \"discussion\", \"do\", \"doubt\", \"even\", \"first\", \"for\", \"from\", \"get\", \"give\", \"god\", \"had\", \"has\", \"have\", \"he\", \"her\", \"here\", \"him\", \"his\", \"history\", \"how\", \"i\", \"if\", \"in\", \"is\", \"it\", \"know\", \"long\", \"man\", \"may\", \"me\", \"meeting\", \"might\", \"must\", \"my\", \"myself\", \"never\", \"new\", \"no\", \"not\", \"of\", \"on\", \"one\", \"or\", \"ought\", \"out\", \"place\", \"see\", \"seen\", \"shall\", \"she\", \"should\", \"so\", \"struck\", \"that\", \"the\", \"there\", \"these\", \"they\", \"think\", \"this\", \"thought\", \"time\", \"to\", \"told\", \"took\", \"university\", \"upon\", \"was\", \"we\", \"were\", \"what\", \"when\", \"will\", \"with\", \"would\", \"you\", \"your\"]\n",
      "\"a\"\t[0, 798276, 791512, 788412, 764521, 792627, 794234, 833985, 796229, 768320, 647878, 786125, 710465, 521557, 794070, 754905, 749330, 764900, 787173, 837420, 769718, 802248, 795733, 811042, 787944, 1862668, 796877, 780269, 710410, 762711, 802547, 789653, 776248, 713039, 790610, 2058416, 721615, 789006, 799248, 772762, 738859, 800474, 788324, 2670655, 787964, 687669, 774348, 752380, 790641, 812607, 792998, 793584, 801733, 601361, 789805, 793478, 766436, 810329, 791726, 795382, 1849013, 923654, 779600, 761425, 772681, 804427, 848145, 794931, 795670, 794080, 793540, 792821, 796415, 792738, 778711, 813273, 1925133, 1022746, 775644, 792996, 774329, 773684, 730022, 781725, 774707, 849746, 792116, 820816, 799304, 824727, 726324, 781699, 796816, 789690, 795294, 788673, 736730, 791371, 757033, 800910]\n",
      "\"about\"\t[798276, 0, 48892, 43992, 63795, 36615, 48614, 226795, 67885, 52494, 294620, 56535, 129069, 527987, 104482, 108289, 313852, 126002, 52427, 94880, 80206, 43970, 39191, 42450, 105554, 1162408, 43251, 79353, 138058, 83673, 43107, 50065, 61532, 205111, 56024, 1513774, 387891, 37854, 31312, 61622, 83289, 49754, 66390, 2528047, 56614, 326905, 164676, 220760, 39629, 46939, 49450, 44438, 46255, 440059, 42955, 35668, 60598, 66297, 45202, 38080, 1208293, 371396, 1086186, 67647, 55531, 36643, 154595, 33855, 76840, 43534, 40972, 41393, 33223, 46326, 60301, 66797, 1450461, 1535772, 69952, 39926, 61985, 69964, 112950, 42745, 97205, 545460, 40766, 42806, 60312, 42327, 198374, 65401, 36686, 43514, 40820, 43127, 108798, 73603, 170339, 34594]\n",
      "\"according\"\t[791512, 48892, 0, 30898, 56167, 47287, 32890, 207027, 43553, 55534, 288818, 55235, 127565, 531385, 109824, 109779, 308558, 127060, 66205, 107430, 95964, 31918, 54339, 37806, 114762, 1174416, 52787, 79721, 133470, 82475, 48317, 57299, 44034, 198755, 52824, 1503494, 384741, 50466, 47146, 70560, 72101, 24980, 83676, 2527755, 75984, 322247, 149900, 230364, 52083, 64619, 54858, 59486, 47817, 411165, 57915, 53828, 72928, 92211, 53630, 50754, 1222721, 376682, 1078708, 63453, 57577, 38339, 147337, 40293, 65496, 58474, 59266, 57107, 52837, 55786, 70793, 63207, 1465207, 1532830, 94750, 36916, 75139, 79076, 113100, 67623, 86995, 546580, 70744, 63124, 37670, 63671, 193524, 77737, 43510, 62420, 54574, 67861, 103328, 101765, 181527, 46740]\n",
      "\"after\"\t[788412, 43992, 30898, 0, 45431, 39787, 33462, 221157, 50053, 52486, 283128, 51863, 120187, 512125, 113908, 100757, 302156, 113438, 59821, 106184, 81274, 29026, 43769, 41286, 116052, 1171940, 38663, 72153, 118086, 69965, 54239, 57581, 49670, 195977, 46752, 1502356, 381601, 40896, 40810, 63002, 70437, 28760, 76574, 2524201, 61662, 312557, 151688, 220124, 48267, 57045, 52394, 53402, 46225, 418849, 50751, 46480, 66654, 89635, 48240, 45286, 1211677, 368428, 1076782, 56283, 49741, 31837, 167327, 35213, 68304, 52336, 51892, 53961, 41281, 51352, 60921, 68295, 1446353, 1528958, 77884, 32416, 65215, 67000, 103574, 53667, 76051, 534250, 67154, 60256, 45782, 61879, 196192, 64361, 34524, 52546, 45164, 60107, 88964, 88681, 178655, 45076]\n",
      "\"all\"\t[764521, 63795, 56167, 45431, 0, 53136, 54325, 230182, 69080, 49475, 257915, 62560, 111908, 508134, 113999, 115802, 283635, 123373, 74656, 127583, 89717, 55329, 56848, 69039, 122399, 1192499, 55664, 93838, 102631, 80712, 71030, 79226, 60269, 201834, 69349, 1495931, 365878, 50001, 57883, 76259, 67778, 53601, 82001, 2493138, 76117, 289326, 137305, 211625, 65996, 80324, 72665, 65731, 63374, 444804, 67454, 59993, 74111, 106808, 60279, 70443, 1216122, 359217, 1045133, 61434, 53470, 54566, 187806, 50050, 93447, 68773, 66999, 64790, 59836, 64001, 68044, 93062, 1436730, 1499055, 81387, 46455, 70472, 75995, 91125, 76242, 94616, 506259, 86563, 80047, 65421, 82438, 190793, 70348, 53989, 57337, 47365, 67988, 95647, 91350, 176808, 59923]\n",
      "\"also\"\t[792627, 36615, 47287, 39787, 53136, 0, 44105, 222034, 65044, 48185, 285943, 55188, 121510, 519062, 102759, 103594, 307883, 120973, 57018, 96601, 72029, 39891, 35192, 48807, 109351, 1162287, 37294, 79388, 123157, 77786, 43520, 50000, 51035, 201186, 48727, 1508137, 384720, 31075, 35673, 47491, 70216, 46993, 69769, 2529334, 58235, 310528, 154951, 216901, 41758, 52848, 42911, 48109, 38976, 437792, 42312, 36011, 61039, 76536, 39441, 43847, 1195712, 360617, 1078685, 61792, 43324, 32564, 156934, 26984, 73899, 44885, 42203, 42666, 38992, 46111, 56808, 66772, 1449002, 1531493, 72911, 38033, 61892, 65421, 108821, 46756, 89920, 537375, 54065, 50545, 58947, 51654, 198755, 59738, 37781, 40735, 37499, 48358, 89947, 73550, 171728, 38299]\n",
      "\"although\"\t[794234, 48614, 32890, 33462, 54325, 44105, 0, 223349, 51937, 58390, 287142, 56829, 122937, 523579, 123696, 106077, 297520, 120556, 73333, 113234, 99874, 36382, 57087, 40784, 128674, 1176236, 51265, 75869, 121826, 81095, 61605, 67881, 57612, 207551, 60596, 1513252, 388771, 54340, 41878, 77592, 80789, 34376, 83792, 2520661, 72818, 313907, 149658, 229236, 59691, 65217, 58070, 68038, 55589, 419429, 62863, 56482, 80730, 95997, 60464, 54420, 1209119, 382410, 1081142, 61321, 57241, 40707, 173229, 43393, 65854, 63370, 59076, 59707, 52591, 60050, 66459, 65803, 1440981, 1533874, 81488, 39134, 78683, 76074, 112998, 66011, 91591, 540120, 69998, 65104, 48264, 64467, 195874, 80195, 45270, 61448, 50348, 69893, 99266, 102839, 190753, 50750]\n",
      "\"am\"\t[833985, 226795, 207027, 221157, 230182, 222034, 223349, 0, 232642, 232341, 383413, 216410, 263192, 659412, 258303, 233834, 333337, 211305, 212386, 181671, 209975, 217457, 213882, 229561, 201353, 1171941, 226722, 232456, 255069, 236128, 223544, 222330, 215311, 262294, 228987, 1410189, 468686, 217205, 230605, 219895, 229212, 224185, 250551, 2445124, 238921, 401500, 277691, 302897, 209864, 246310, 236475, 224945, 224806, 588828, 209574, 218529, 203415, 232208, 222043, 219631, 1213872, 320645, 1147381, 214852, 234856, 221390, 227896, 221250, 229441, 216459, 229461, 212374, 229452, 218537, 230970, 197152, 1355134, 1418207, 241753, 223515, 236886, 216995, 258563, 233500, 211796, 492747, 226469, 234741, 232579, 235868, 280449, 243918, 218001, 225937, 223097, 226512, 231003, 256354, 251956, 219073]\n",
      "\"american\"\t[796229, 67885, 43553, 50053, 69080, 65044, 51937, 232642, 0, 68911, 291921, 72384, 141490, 542050, 139749, 130056, 316863, 142463, 84126, 127455, 113901, 49759, 70288, 59397, 143523, 1193681, 69360, 77166, 142083, 102940, 75324, 83220, 69207, 230898, 78921, 1527603, 392368, 66413, 62485, 84777, 82554, 40019, 105525, 2535694, 92477, 321198, 169809, 252481, 73928, 82346, 76955, 82703, 68184, 432922, 76750, 69817, 88575, 113884, 76979, 65987, 1230734, 408415, 1058093, 78656, 70038, 56420, 187122, 57118, 63853, 78083, 74037, 70584, 68766, 77803, 84116, 84286, 1476210, 1517545, 108625, 51851, 91986, 98329, 126501, 86394, 111460, 560171, 90175, 80989, 38005, 83384, 208285, 93238, 59481, 76667, 70791, 83046, 114187, 118718, 204810, 65105]\n",
      "\"an\"\t[768320, 52494, 55534, 52486, 49475, 48185, 58390, 232341, 68911, 0, 265350, 61047, 101205, 509759, 104484, 115813, 293340, 126596, 68685, 119848, 88342, 59858, 56215, 73906, 113478, 1186054, 57037, 94825, 108228, 86049, 60659, 67067, 64054, 207395, 66708, 1506526, 367921, 43782, 55328, 63530, 75109, 49004, 83460, 2499459, 73398, 294635, 151542, 218626, 57907, 74535, 58288, 61472, 61889, 434493, 62287, 54332, 68118, 97129, 55226, 56252, 1214693, 366626, 1057320, 61153, 43979, 51153, 175451, 47135, 95908, 65124, 61640, 58023, 56933, 60756, 59967, 88547, 1445303, 1511102, 81922, 42216, 78215, 74664, 94310, 65837, 103643, 511280, 76006, 70222, 59242, 71945, 193534, 68687, 52312, 64382, 60176, 65755, 101530, 98237, 182179, 49950]\n",
      "removing tmp directory /tmp/HW542.hduser.20151008.041033.665064\n",
      "Traceback (most recent call last):\n",
      "  File \"./HW542.py\", line 65, in <module>\n",
      "    HW542.run()\n",
      "  File \"/home/hduser/anaconda/lib/python2.7/site-packages/mrjob/job.py\", line 461, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/hduser/anaconda/lib/python2.7/site-packages/mrjob/job.py\", line 479, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/hduser/anaconda/lib/python2.7/site-packages/mrjob/launch.py\", line 153, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/hduser/anaconda/lib/python2.7/site-packages/mrjob/launch.py\", line 220, in run_job\n",
      "    self.stdout.write(line)\n",
      "IOError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!./HW542.py hw541_sample_output --number_unigrams=100 | head -11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, $\\text{distance}(\\text{unigram}_i,\\text{unigram}_i)=0.0 \\text{ } \\forall i \\in \\{1,N\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW54_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW542_driver.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "from HW542 import HW542\n",
    "\n",
    "import os\n",
    "\n",
    "mr_job = HW542(args=['s3://ucb-mids-mls-juanjocarin/Confidence.txt', \n",
    "                     '-r', 'emr', '--number_unigrams=10000',\n",
    "                     '--output-dir=s3://ucb-mids-mls-juanjocarin/Distances',\n",
    "                     '--no-output'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "os.system(\"aws s3 cp s3://ucb-mids-mls-juanjocarin/Distances/part-00000 \\\n",
    "    s3://ucb-mids-mls-juanjocarin/Distances.txt\")\n",
    "os.system(\"aws s3 rm s3://ucb-mids-mls-juanjocarin/Distances/part-00000\")\n",
    "os.system(\"aws s3 rm s3://ucb-mids-mls-juanjocarin/Distances/_SUCCESS\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x HW542_driver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./HW542_driver.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A client error (404) occurred when calling the HeadObject operation: Key \"Stripe_commparison.txt\" does not exist\n",
      "Completed 1 part(s) with ... file(s) remaining\n",
      "A client error (404) occurred when calling the HeadObject operation: Key \"Stripe_commparison.txt\" does not exist\n",
      "Completed 1 part(s) with ... file(s) remaining\n",
      "head: cannot open Stripe_commparison.txt for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://ucb-mids-mls-juanjocarin/Distances.txt ./Distances.txt\n",
    "!aws s3 rm s3://ucb-mids-mls-juanjocarin/Distances.txt\n",
    "!head -25 Distances.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
